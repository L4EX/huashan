{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54017e2a-395d-423f-9767-4725bf59d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238f7e3f-0e89-4b2e-be56-46077fc6e227",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (579327406.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 37\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(df['Column1'])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# pandas常用操作\n",
    "import pandas as pd\n",
    "\n",
    "# 使用列表创建df,index可以不声明\n",
    "data = [['Google', 10], ['Runoob', 12], ['Wiki', 13]]\n",
    "df = pd.DataFrame(data, columns=['Site', 'Age'], index = [\"1\", \"2\", \"3\"])\n",
    "\n",
    "# 使用astype方法设置每列的数据类型\n",
    "df['Site'] = df['Site'].astype(str)\n",
    "df['Age'] = df['Age'].astype(float)\n",
    "\n",
    "# 使用字典创建df\n",
    "data = {'Site':['Google', 'Runoob', 'Wiki'], 'Age':[10, 12, 13]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#切片\n",
    "# 返回第i行，得到一个series数据\n",
    "df.loc[0]\n",
    "\n",
    "# 返回第一行和第二行\n",
    "df.loc[[0, 1]]\n",
    "\n",
    "# DataFrame 的属性和方法\n",
    "print(df.shape)     # 形状\n",
    "print(df.columns)   # 列名\n",
    "print(df.index)     # 索引\n",
    "print(df.head())    # 前几行数据，默认是前 5 行\n",
    "print(df.tail())    # 后几行数据，默认是后 5 行\n",
    "print(df.info())    # 数据信息\n",
    "print(df.describe())# 描述统计信息\n",
    "print(df.mean())    # 求平均值\n",
    "print(df.sum())     # 求和\n",
    "\n",
    "#访问列\n",
    "\n",
    "    # 通过列名访问\n",
    "    print(df['Column1'])\n",
    "    \n",
    "    # 通过属性访问\n",
    "    print(df.Name)    \n",
    "       \n",
    "    # 通过 .loc[] 访问\n",
    "    print(df.loc[:, 'Column1'])\n",
    "    \n",
    "    # 通过 .iloc[] 访问\n",
    "    print(df.iloc[:, 0])  # 假设 'Column1' 是第一列\n",
    "    \n",
    "    # 访问单个元素\n",
    "    print(df['Name'][0])\n",
    "\n",
    "#访问行\n",
    "    df[:10]\n",
    "\n",
    "# 纵向合并\n",
    "df = pd.concat([df, new_row], ignore_index=True)  # 将新行添加到原始DataFrame\n",
    "# 横向合并\n",
    "df = pd.merge(df1, df2, on='Column1')\n",
    "\n",
    "# 删除\n",
    "# 删除列：使用 drop 方法。\n",
    "df_dropped = df.drop('Column1', axis=1)\n",
    "\n",
    "# 删除行：同样使用 drop 方法。\n",
    "df_dropped = df.drop(0)  # 删除索引为 0 的行\n",
    "\n",
    "#DataFrame 的统计分析\n",
    "#描述性统计：使用 .describe() 查看数值列的统计摘要。\n",
    "df.describe()\n",
    "\n",
    "#计算统计数据：使用聚合函数如 .sum()、.mean()、.max() 等。\n",
    "df['Column1'].sum()\n",
    "df.mean()\n",
    "\n",
    "# DataFrame 的索引操作\n",
    "# 重置索引：使用 .reset_index()。\n",
    "df_reset = df.reset_index(drop=True)\n",
    "\n",
    "# 设置索引：使用 .set_index()。\n",
    "df_set = df.set_index('Column1')\n",
    "\n",
    "# DataFrame 的布尔索引\n",
    "# 使用布尔表达式：根据条件过滤 DataFrame。\n",
    "df[df['Column1'] > 2]\n",
    "\n",
    "# DataFrame 的数据类型\n",
    "#查看数据类型：使用 dtypes 属性。\n",
    "df.dtypes\n",
    "\n",
    "# 转换数据类型：使用 astype 方法。\n",
    "df['Column1'] = df['Column1'].astype('float64')\n",
    "\n",
    "# 索引和切片\n",
    "print(df[['Name', 'Age']])  # 提取多列\n",
    "print(df[1:3])               # 切片行\n",
    "print(df.loc[:, 'Name'])     # 提取单列\n",
    "print(df.loc[1:2, ['Name', 'Age']])  # 标签索引提取指定行列\n",
    "print(df.iloc[:, 1:])        # 位置索引提取指定列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb891f7e-73af-4adb-87ac-3f436c4c6cd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pandas数据清洗\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 删除包含空字段的行，使用 dropna() 方法：\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m DataFrame\u001b[38;5;241m.\u001b[39mdropna(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m, thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataFrame' is not defined"
     ]
    }
   ],
   "source": [
    "# pandas数据清洗\n",
    "\n",
    "# 空值处理\n",
    "\n",
    "# 常见空数据\n",
    "n/a\n",
    "NA\n",
    "—\n",
    "na\n",
    "\n",
    "# 删除包含空字段的行，使用 dropna() 方法：\n",
    "df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "\n",
    "# 使用isnull()判断某列中的每个值是否为空\n",
    "df['NUM_BEDROOMS'].isnull()\n",
    "\n",
    "# 读取df时，可以将多种空数据指定为无效值NaN\n",
    "missing_values = [\"n/a\", \"na\", \"--\"]\n",
    "df = pd.read_csv('property-data.csv', na_values = missing_values)\n",
    "\n",
    "# 使用统计值替换空值\n",
    "x = df[\"ST_NUM\"].mean()  # .median() .mode()\n",
    "df[\"ST_NUM\"].fillna(x, inplace = True)\n",
    "\n",
    "# 格式统一\n",
    "# 第三个日期格式错误\n",
    "data = {\n",
    "  \"Date\": ['2020/12/01', '2020/12/02' , '20201226'],\n",
    "  \"duration\": [50, 40, 45]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index = [\"day1\", \"day2\", \"day3\"])\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='mixed')\n",
    "\n",
    "# 使用条件判断修改值\n",
    "person = {\n",
    "  \"name\": ['Google', 'Runoob' , 'Taobao'],\n",
    "  \"age\": [50, 200, 12345]    \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(person)\n",
    "\n",
    "for x in df.index:\n",
    "  if df.loc[x, \"age\"] > 120:\n",
    "    df.loc[x, \"age\"] = 120\n",
    "    \n",
    "# 清洗重复数据\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "# group_by高级用法 https://zhuanlan.zhihu.com/p/607906239\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debbf6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据相关性分析及可视化\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 创建一个示例数据框\n",
    "data = {'A': [1, 2, 3, 4, 5], 'B': [5, 4, 3, 2, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 计算 Pearson 相关系数\n",
    "correlation_matrix = df.corr()\n",
    "# 使用热图可视化 Pearson 相关系数\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pandas as pd\n",
    "df = pd.read_csv(r\"/Users/L4EX/Downloads/AI_Risk-master/B榜code/data/age.csv\")\n",
    "df.to_csv(r\"/Users/L4EX/Downloads/AI_Risk-master/B榜code/data/age.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35415f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 数据预处理\n",
    "# 1 缺失值处理\n",
    "\n",
    "# 1、1 删除含有缺失值的记录\n",
    "\n",
    "# 1、2 使用统计值填充\n",
    "\n",
    "# 1、3 使用聚类算法预测\n",
    "\n",
    "# 2 异常值检测\n",
    "\n",
    "# 2、1 使用统计方法检测（Z-Score、IQR）\n",
    "\n",
    "# 3 数据标准化/归一化\n",
    " \n",
    "# 3、1 标准化\n",
    "\n",
    "# 3、2 归一化\n",
    "\n",
    "# 4 编码分类变量\n",
    "\n",
    "# 4、1 one-hot独热\n",
    "enc = OneHotEncoder(categories='auto').fit(X)\n",
    "enc.transform(X)\n",
    "\n",
    "result = enc.transform(X)\n",
    "    #用pandas也可以实现\n",
    "        dummy_fea = ['sex', 'qq_bound', 'wechat_bound','account_grade']\n",
    "        dummy_df = pd.get_dummies(train_data.loc[:,dummy_fea])\n",
    "        train_data_copy = pd.concat([train_data,dummy_df],axis=1)\n",
    "        train_data_copy = train_data_copy.fillna(0)\n",
    "        vaild_train_data = train_data_copy.drop(dummy_fea,axis=1)\n",
    "\n",
    "# 4、2 标签专用，能够将分类转换为分类数值\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "y = data_.iloc[:, -1]  #要输入的是标签，不是特征矩阵，所以允许一维\n",
    "le = LabelEncoder()  #实例化\n",
    "le = le.fit(y)  #导入数据\n",
    "label = le.transform(y)  #transform接口调取结果\n",
    "\n",
    "    #或\n",
    "    data.iloc[:,-1] = LabelEncoder().fit_transform(data.iloc[:,-1])\n",
    "\n",
    "# 4、3 特征专用，能够将分类特征转换为分类数值\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "data_.iloc[:, 1:-1] = OrdinalEncoder().fit_transform(data_.iloc[:, 1:-1])\n",
    "\n",
    "\n",
    "# 特选择和降维\n",
    "\n",
    "\n",
    "# 划分训练集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=8)\n",
    "\n",
    "# 模型保存\n",
    "model.save_model('D:/MachineLearning/ofo/ofoOptimization/xgbmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 画图可视化\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 导入需要的包\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, auc, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "# 加载数据集，这里直接使用datasets包里面的乳腺癌分类数据（二分类）\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "feature_names = list(feature_names)\n",
    "\n",
    "# 输出数据集的形状，该数据集里面有569个样本，每个样本有30个特征(569, 30)\n",
    "print(X.shape)\n",
    "# 输出标签的个数为 569\n",
    "print(y.shape)\n",
    "\n",
    "# 使用train_test_split()函数对训练集和测试集进行划分，第一个参数是数据集特征，第二个参数是标签，第三个为测试集占总样本的百分比\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 6)\n",
    "\n",
    "# 使用XGBoost进行训练\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "model.get_booster().feature_names = feature_names\n",
    "\n",
    "# 绘制重要性曲线, max_num_feature参数设置输出前30重要的特征,【数据集中共有30个特征】\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "plot_importance(model, max_num_features=30, ax=ax)\n",
    "plt.savefig(\"demo_plot_importance.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# 类别值\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# 输出ACC的值\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"acc:\", acc)\n",
    "# 输出recall值\n",
    "re = recall_score(y_test, y_pred)\n",
    "print(\"recall:\", re)\n",
    "# 输出precision\n",
    "pre = precision_score(y_test, y_pred)\n",
    "print(\"precision:\", pre)\n",
    "# 输出f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "\n",
    "# 概率得分\n",
    "y_score = model.predict_proba(x_test)[:,1]\n",
    "\n",
    "# 直接计算auc的值\n",
    "auc_1 = roc_auc_score(y_test, y_score)\n",
    "print(\"auc_1:\", auc_1)\n",
    "\n",
    "# 绘制ROC曲线\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_score)\n",
    "# 间接计算auc的值\n",
    "auc_2 = auc(fpr, tpr) \n",
    "print(\"auc_2:\", auc_2)\n",
    "\n",
    "# 间接计算auc的值的好处，就是可以知道fpr和tpr，绘制曲线\n",
    "plt.plot(fpr,tpr,'r--', label='auc=%0.4f'%auc_2)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"demo_roc.png\",dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# 绘制PR曲线\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, y_score)\n",
    "aupr = auc(recall, precision)\n",
    "print(\"aupr:\", aupr)\n",
    "plt.plot(recall, precision, 'g--', label='aupr=%0.4f'%aupr)\n",
    "plt.title(\"PR Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"demo_pr.png\",dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785936b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例\n",
    "\n",
    "一、读取数据并指定参数建模\n",
    "xgboost读取数据有两种方式：\n",
    "\n",
    "使用xgboost自带的数据集格式 + xgboost自带的建模方式\n",
    "把数据读取成xgb.DMatrix格式(libsvm/dataframe.values给定X和Y)\n",
    "准备好一个watch_list(观测和评估的数据集)\n",
    "xgb.train(dtrain)\n",
    "xgb.predict(dtest)\n",
    "使用pandas的DataFrame格式 + xgboost的sklearn接口\n",
    "estimator = xgb.XGBClassifier()/xgb.XGBRegressor()\n",
    "estimator.fit(df_train.values, df_target.values) 先看一下第一种方式读取数据和建模的方法。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('./data/Pima-Indians-Diabetes.csv')\n",
    "\n",
    "train, test = train_test_split(data)\n",
    "\n",
    "# 转换成Dmatrix格式\n",
    "feature_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "target_column = 'Outcome'\n",
    "# 需要将dataframe格式的数据转化为矩阵形式\n",
    "xgtrain = xgb.DMatrix(train[feature_columns].values, train[target_column].values)\n",
    "xgtest = xgb.DMatrix(test[feature_columns].values, test[target_column].values)\n",
    "\n",
    "#参数设定\n",
    "param = {'max_depth':5, 'eta':0.1, 'silent':1, 'subsample':0.7, 'colsample_bytree':0.7, 'objective':'binary:logistic' }\n",
    "\n",
    "# 设定watchlist用于查看模型状态\n",
    "watchlist  = [(xgtest,'eval'), (xgtrain,'train')]\n",
    "num_round = 10\n",
    "bst = xgb.train(param, xgtrain, num_round, watchlist)\n",
    "\n",
    "# 使用模型预测\n",
    "preds = bst.predict(xgtest)\n",
    "\n",
    "# 判断准确率\n",
    "labels = xgtest.get_label()\n",
    "print ('错误类为%f' % \\\n",
    "       (sum(1 for i in range(len(preds)) if int(preds[i]>0.5)!=labels[i]) /float(len(preds))))\n",
    "\n",
    "# 模型存储\n",
    "bst.save_model('./model/0002.model')\n",
    "\n",
    "　第一点就是输入数据形式要转化成矩阵的形式，第二点就是watchlist参数用于查看模型的状态，也就是为了输出eval-error和train-error。然后再解释下几个参数的含义：\n",
    "\n",
    "'max_depth':设置树的最大深度。默认为6。\n",
    "'eta'：学习率。默认为0.3。\n",
    "'silent':0表示输出信息， 1表示安静模式。默认为0。\n",
    "'subsample':观测的子样本的比率，即对总体进行随机抽样的比例。默认为1。\n",
    "'colsample_bytree ':用于构造每棵树时变量的子样本比率.即特征抽样。默认为1。\n",
    "'objective':最小化的损失函数。\n",
    "　xgboost的参数可以分为三类，通用参数/general parameters, 集成(增强)参数/booster parameters 和 任务参数/task parameters。 以上silent是通用参数，objective是任务参数，其它的都是集成参数。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "　再来看一下使用sklearn接口进行建模的例子。\n",
    "\n",
    "#!/usr/bin/python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "data = pd.read_csv('./data/Pima-Indians-Diabetes.csv')\n",
    "\n",
    "train, test = train_test_split(data)\n",
    "\n",
    "# 取出特征X和目标y的部分\n",
    "feature_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "target_column = 'Outcome'\n",
    "train_X = train[feature_columns].values\n",
    "train_y = train[target_column].values\n",
    "test_X = test[feature_columns].values\n",
    "test_y = test[target_column].values\n",
    "\n",
    "# 初始化模型\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=20,max_depth=4,learning_rate=0.1, subsample=0.7, colsample_bytree=0.7)\n",
    "\n",
    "# 拟合模型\n",
    "xgb_classifier.fit(train_X, train_y)\n",
    "\n",
    "# 使用模型预测\n",
    "preds = xgb_classifier.predict(test_X)\n",
    "\n",
    "# 判断准确率\n",
    "print ('错误类为%f' %((preds!=test_y).sum()/float(test_y.shape[0])))\n",
    "\n",
    "# 模型存储\n",
    "joblib.dump(xgb_classifier, './model/0003.model')\n",
    "读取数据和建模的大致流程就这么两种，下面结合xgb建模的方式进行深入地了解。\n",
    "利用xgb进行交叉验证。\n",
    "\n",
    "param = {'max_depth':5, 'eta':0.1, 'silent':0, 'subsample':0.7, 'colsample_bytree':0.7, 'objective':'binary:logistic' }\n",
    "num_round = 10\n",
    "dtrain = xgb.DMatrix(train[feature_columns].values, train[target_column].values)\n",
    "\n",
    "xgb.cv(param, dtrain, num_round, nfold=5,metrics={'error'}, seed = 0)\n",
    "\n",
    "xgb.cv的参数的含义如下：\n",
    "\n",
    "'num_round':最大迭代次数。\n",
    "'metric':评价指标，一般用AUC。\n",
    "　除了xgb自带的验证方法以外还有GridSearchCV交叉验证方法，后面会提到。然后进行添加预处理的交叉验证，即通过计算正负样本比调整样本的权重。\n",
    "\n",
    "# 计算正负样本比，调整样本权重\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label==1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    return (dtrain, dtest, param)\n",
    "\n",
    "# 先做预处理，计算样本权重，再做交叉验证\n",
    "xgb.cv(param, dtrain, num_round, nfold=5,\n",
    "       metrics={'auc'}, seed = 0, fpreproc = fpreproc)\n",
    "在参数里面加了一个'scale_pos_weight',可以在样本类别不平衡的时候加速收敛。\n",
    "下面再进行自定义损失函数与评估准则，这也是xgb的优势所在。\n",
    "\n",
    "print ('running cross validation, with cutomsized loss function')\n",
    "# 自定义损失函数，需要提供损失函数的一阶导和二阶导\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = 1.0 / (1.0 + np.exp(-preds))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1.0-preds)\n",
    "    return grad, hess\n",
    "\n",
    "# 自定义评估准则，评估预估值和标准答案之间的差距\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'error', float(sum(labels != (preds > 0.0))) / len(labels)\n",
    "\n",
    "watchlist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "param = {'max_depth':3, 'eta':0.1, 'silent':1}\n",
    "num_round = 5\n",
    "# 自定义损失函数训练\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist, logregobj, evalerror)\n",
    "# 交叉验证\n",
    "xgb.cv(param, dtrain, num_round, nfold = 5, seed = 0,\n",
    "       obj = logregobj, feval=evalerror)\n",
    "　针对业务改写评价函数，这里的要求是要保证损失函数二阶可导，原因只要推导一遍xgboost公式就理解了，因为其中有一步用到泰勒展开的二阶项。\n",
    "　此外，xgb还可以只用前n棵树进行预测，用到的参数是ntree_limit，不再赘述。\n",
    "\n",
    "　了解完xgb进行建模之后，再看一下使用sklearn进行xgb建模的实例。这里使用了鸢尾花数据集合波士顿房价预测数据集分别进行分类和回归的学习。\n",
    "\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.datasets import load_iris, load_digits, load_boston\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "分类：\n",
    "\n",
    "#二分类：混淆矩阵\n",
    "print(\"数字0和1的二分类问题\")\n",
    "digits = load_digits(2)\n",
    "y = digits['target']\n",
    "X = digits['data']\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "print(\"在2折数据上的交叉验证\")\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xgb_model = xgb.XGBClassifier().fit(X[train_index],y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(\"混淆矩阵:\")\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "\n",
    "#多分类：混淆矩阵\n",
    "print(\"\\nIris: 多分类\")\n",
    "iris = load_iris()\n",
    "y = iris['target']\n",
    "X = iris['data']\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "print(\"在2折数据上的交叉验证\")\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xgb_model = xgb.XGBClassifier().fit(X[train_index],y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(\"混淆矩阵:\")\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "\n",
    "回归：\n",
    "\n",
    "#回归问题：MSE\n",
    "print(\"\\n波士顿房价回归预测问题\")\n",
    "boston = load_boston()\n",
    "y = boston['target']\n",
    "X = boston['data']\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "print(\"在2折数据上的交叉验证\")\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xgb_model = xgb.XGBRegressor().fit(X[train_index],y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(\"MSE:\",mean_squared_error(actuals, predictions))\n",
    "\n",
    "　接下来看一下如何利用网格搜索查找最优超参数。这种方法仅适用于使用sklearn接口建模，采用GridSearchCV方法。\n",
    "\n",
    "#调参方法：使用sklearn接口的regressor + GridSearchCV\n",
    "print(\"参数最优化：\")\n",
    "y = boston['target']\n",
    "X = boston['data']\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "param_dict = {'max_depth': [2,4,6],\n",
    "              'n_estimators': [50,100,200]}\n",
    "\n",
    "clf = GridSearchCV(xgb_model, param_dict, verbose=1)\n",
    "clf.fit(X,y)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)\n",
    "\n",
    "　可以看到网格搜索结果，最大深度为2，100个估计器。网格调参的方法就是将参数及参数的取值放入一个字典中，然后作为GridSearchCV这个方法的参数。\n",
    "　再看一下xgboost如何采用早停的参数停止树的增长。早停参数的调整适用于两种xgb建模方法。\n",
    "\n",
    "# 第1/2种训练方法的 调参方法：early stopping\n",
    "X = digits['data']\n",
    "y = digits['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train, early_stopping_rounds=20, eval_metric=\"auc\",\n",
    "        eval_set=[(X_val, y_val)])\n",
    "\n",
    "　该方法在训练集上学习模型，一颗一颗树添加，在验证集上看效果，当验证集效果不再提升，停止树的添加与生长。可以看到第10棵树时就不再提高，所以early_stopping_rounds的最佳值为10。关于xgboost调参的方法可以网上找一篇教程看一下，方法都是一样的，看一遍就懂了，附上很久之前整理的调参的基本思路。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "　最后再画图看一下特征的重要性。\n",
    "\n",
    "iris = load_iris()\n",
    "y = iris['target']\n",
    "X = iris['data']\n",
    "xgb_model = xgb.XGBClassifier().fit(X,y)\n",
    "\n",
    "print('特征排序：')\n",
    "feature_names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "for index in indices:\n",
    "    print(\"特征 %s 重要度为 %f\" %(feature_names[index], feature_importances[index]))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title(\"feature importances\")\n",
    "plt.bar(range(len(feature_importances)), feature_importances[indices], color='b')\n",
    "plt.xticks(range(len(feature_importances)), np.array(feature_names)[indices], color='b')\n",
    "　xgboost特征重要性指标: weight, gain, cover,可以通过xgb_model.get_booster().get_score(importance_type=importance_type))中的importance进行设置。\n",
    "　最后附上一个并行训练加速的代码，需要用到的时候再来看。\n",
    "\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from multiprocessing import set_start_method\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Unable to import multiprocessing.set_start_method.\"\n",
    "                          \" This example only runs on Python 3.4\")\n",
    "    # set_start_method(\"forkserver\")\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.datasets import load_boston\n",
    "    import xgboost as xgb\n",
    "\n",
    "    rng = np.random.RandomState(31337)\n",
    "\n",
    "    print(\"Parallel Parameter optimization\")\n",
    "    boston = load_boston()\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"2\"  # or to whatever you want\n",
    "    y = boston['target']\n",
    "    X = boston['data']\n",
    "    xgb_model = xgb.XGBRegressor()\n",
    "    clf = GridSearchCV(xgb_model, {'max_depth': [2, 4, 6],\n",
    "                                   'n_estimators': [50, 100, 200]}, verbose=1,\n",
    "                       n_jobs=2)\n",
    "    clf.fit(X, y)\n",
    "    print(clf.best_score_)\n",
    "    print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 风险预测实战\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "def xgb_valid(train_set_x,train_set_y):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective':'rank:pairwise',\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.02,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,#0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 1,  # 2 3\n",
    "              'silent':1,\n",
    "              'nthread':8\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(train_set_x, label=train_set_y)\n",
    "    model = xgb.cv(params, dtrain, num_boost_round=1000,nfold=5,metrics={'auc'},seed=10)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "def xgb_feature(train_set_x,train_set_y,test_set_x,test_set_y):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective':'rank:pairwise',\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.02,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,#0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 1,  # 2 3\n",
    "              'silent':1\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(train_set_x, label=train_set_y)\n",
    "    dvali = xgb.DMatrix(test_set_x)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=800)\n",
    "    predict = model.predict(dvali)\n",
    "    return predict\n",
    "\n",
    "IS_OFFLine = False\n",
    "if __name__ == '__main__':\n",
    "    #%%认证表特征\n",
    "    train_auth = pd.read_csv('../AI_risk_train_V3.0/train_auth_info.csv',parse_dates = ['auth_time'])\n",
    "    #注册时是否有时间\n",
    "    train_auth['is_auth_time_authtable'] = train_auth['auth_time'].map(lambda x:0 if str(x)=='nan' else 1)\n",
    "    #注册时是否有idcard\n",
    "    train_auth['is_idcard_authtable'] = train_auth['id_card'].map(lambda x:0 if str(x)=='nan' else 1)\n",
    "\n",
    "    #注册时是否有phone\n",
    "    train_auth['is_phone_authtable'] = train_auth['phone'].map(lambda x:0 if str(x)=='nan' else 1)\n",
    "\n",
    "\n",
    "    #%%银行卡特征\n",
    "    train_bankcard = pd.read_csv('../AI_risk_train_V3.0/train_bankcard_info.csv')\n",
    "    bank_name_setlen = train_bankcard.groupby(by= ['id'], as_index= False)['bank_name'].agg({'bank_name_len':lambda x:len(set(x))})\n",
    "    bank_num_len = train_bankcard.groupby(by= ['id'], as_index = False)['tail_num'].agg({'tail_num_len':lambda x:len(x)})\n",
    "    bank_phone_num_setlen = train_bankcard.groupby(by= ['id'], as_index = False)['phone'].agg({'bank_phone_num':lambda x:x.nunique()})\n",
    "    \n",
    "    train_bankcard['card_type_score'] = train_bankcard['card_type'].map(lambda x:0.0154925 if x=='信用卡' else 0.02607069)\n",
    "    bank_card_type_score = train_bankcard.groupby(by= ['id'], as_index = False)['card_type_score'].agg({'card_type_score_mean':np.mean})\n",
    " \n",
    "    #%%信誉表特征\n",
    "    train_credit = pd.read_csv('../AI_risk_train_V3.0/train_credit_info.csv')\n",
    "    #额度-使用值\n",
    "    train_credit['can_use_credittable'] = train_credit['quota'] - train_credit['overdraft']\n",
    "\n",
    "    #%%订单表特征\n",
    "    train_order = pd.read_csv('../AI_risk_train_V3.0/train_order_info.csv',parse_dates=['time_order'])\n",
    "    train_order['amt_order_ordertable'] = train_order['amt_order'].map(lambda x:np.nan if ((x == 'NA')| (x == 'null')) else float(x))\n",
    "    train_order['unit_price_ordertable'] = train_order['unit_price'].map(lambda x:np.nan if ((x == 'NA')| (x == 'null')) else float(x))\n",
    "    \n",
    "    train_order['time_order_ordertable'] = train_order['time_order'].map(lambda x : pd.lib.NaT if (str(x) == '0' or x == 'NA' or x == 'nan')\n",
    "                                else (datetime.datetime.strptime(str(x),'%Y-%m-%d %H:%M:%S') if ':' in str(x)\n",
    "                                else (datetime.datetime.utcfromtimestamp(int(x[0:10])) + datetime.timedelta(hours = 8))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%%收货地址特征\n",
    "    train_recieve = pd.read_csv('../AI_risk_train_V3.0/train_recieve_addr_info.csv')\n",
    "    train_recieve['first_name'] = train_recieve['region'].map(lambda x:str(x)[:2])\n",
    "    train_recieve['last_name'] = train_recieve['region'].map(lambda x:str(x)[-1])\n",
    "\n",
    "\n",
    "    #%%target表特征\n",
    "    train_target = pd.read_csv('../AI_risk_train_V3.0/train_target.csv',parse_dates = ['appl_sbm_tm'])\n",
    "\n",
    "\n",
    "    #%%用户表特征\n",
    "    train_user = pd.read_csv('../AI_risk_train_V3.0/train_user_info.csv')\n",
    "    train_user = train_user.drop(['merriage','income','id_card','degree','industry'],axis=1)\n",
    "    # train_user = train_user.drop(['id_card'],axis=1)\n",
    "    train_user['is_hobby_usertable'] = train_user['hobby'].map(lambda x:0 if str(x)=='nan' else 1)\n",
    "    train_user['is_birthday_usertable'] = train_user['birthday'].map(lambda x:0 if str(x)=='nan' else 1)\n",
    "    train_user['birthday'] = train_user['birthday'].map(lambda x:datetime.datetime.strptime(str(x),'%Y-%m-%d') if(re.match('19\\d{2}-\\d{1,2}-\\d{1,2}',str(x)) and '-0' not in str(x)) else pd.lib.NaT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_data = pd.merge(train_target,train_auth,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,train_user,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,train_credit,on=['id'],how='left')\n",
    "    train_data['loan_hour'] = train_data['appl_sbm_tm'].map(lambda x:x.hour)\n",
    "    train_data['loan_day'] = train_data['appl_sbm_tm'].map(lambda x:x.day)\n",
    "    train_data['loan_month'] = train_data['appl_sbm_tm'].map(lambda x:x.month)\n",
    "    train_data['loan_year'] = train_data['appl_sbm_tm'].map(lambda x:x.year)\n",
    "    train_data['nan_num'] = train_data.isnull().sum(axis=1)\n",
    "    train_data['diff_day'] = train_data.apply(lambda row: (row['appl_sbm_tm'] - row['auth_time']).days,axis=1)\n",
    "    train_data['how_old'] = train_data.apply(lambda row: (row['appl_sbm_tm'] - row['birthday']).days,axis=1)\n",
    "    train_data['是否认证时间在借贷时间前'] = train_data.apply(lambda x:0 if (x['is_auth_time_authtable'] == 0) else ( 1 if x['auth_time'] < x['appl_sbm_tm'] else 0),axis=1)\n",
    "    train_data['是否认证时间在借贷时间后'] = train_data.apply(lambda x:0 if (x['is_auth_time_authtable'] == 0) else ( 1 if x['auth_time'] > x['appl_sbm_tm'] else 0),axis=1)\n",
    "    train_data['认证时间在借贷时间前多少天'] = train_data.apply(lambda x:0 if (x['是否认证时间在借贷时间前'] == 0) else (x['appl_sbm_tm'] - x['auth_time']).days,axis=1)\n",
    "    train_data['认证时间在借贷时间后多少天'] = train_data.apply(lambda x:0 if (x['是否认证时间在借贷时间后'] == 0) else (x['auth_time'] - x['appl_sbm_tm']).days,axis=1)\n",
    "    train_data = pd.merge(train_data,bank_name_setlen,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,bank_num_len,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,bank_phone_num_setlen,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,bank_card_type_score,on=['id'],how='left')\n",
    "\n",
    "    #%%为订单表建立临时表1\n",
    "    tmp_train_order = pd.merge(train_order, train_target, on = ['id'])\n",
    "    tmp_train_order_before_appl_sbm_tm = tmp_train_order[tmp_train_order.time_order_ordertable < tmp_train_order.appl_sbm_tm]\n",
    "    tmp_train_order_after_appl_sbm_tm = tmp_train_order[tmp_train_order.time_order_ordertable > tmp_train_order.appl_sbm_tm]\n",
    "    before_appl_sbm_tm_howmany = tmp_train_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间前有多少次购买':len})\n",
    "    after_appl_sbm_tm_howmany = tmp_train_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间后有多少次购买':len})\n",
    "    before_appl_sbm_tm_money_mean = tmp_train_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间前有多少次购买':np.mean})\n",
    "    after_appl_sbm_tm_money_mean = tmp_train_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间后有多少次购买':np.mean})\n",
    "    before_appl_sbm_tm_money_max = tmp_train_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间前有多少次购买最大值':np.max})\n",
    "    after_appl_sbm_tm_money_min = tmp_train_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间后有多少次购买最小值':np.min})\n",
    "\n",
    "\n",
    "    # before_appl_sbm_tm_howmany_unitprice = tmp_train_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间前有多少次购买unit_price':len})\n",
    "    # after_appl_sbm_tm_howmany_unitprice = tmp_train_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间后有多少次购买unit_price':len})\n",
    "    # before_appl_sbm_tm_money_mean_unitprice = tmp_train_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间前有多少次购买unit_price':np.mean})\n",
    "    # after_appl_sbm_tm_money_mean_unitprice = tmp_train_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间后有多少次购买unit_price':np.mean})\n",
    "    # before_appl_sbm_tm_money_max_unitprice = tmp_train_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间前有多少次购买最大值unit_price':np.max})\n",
    "    # after_appl_sbm_tm_money_min_unitprice = tmp_train_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间后有多少次购买最小值unit_price':np.min})\n",
    "\n",
    "\n",
    "    ##建立银行违约率临时表\n",
    "    tmp_bank_target = pd.merge(train_bankcard,train_target,on=['id'])\n",
    "    ccc = pd.crosstab(tmp_bank_target.bank_name, tmp_bank_target.target)\n",
    "    ccc['违约率'] = ccc[1] / (ccc[0]+0.1)\n",
    "    ccc.reset_index(inplace=True)\n",
    "    tmp_bank_target = pd.merge(tmp_bank_target, ccc,on = ['bank_name'],how='left')\n",
    "    bank_name_score_mean = tmp_bank_target.groupby(by= ['id'], as_index = False)['违约率'].agg({'违约率_mean':np.mean})\n",
    "\n",
    "    train_data = pd.merge(train_data,bank_name_score_mean,on=['id'],how='left')\n",
    "\n",
    "\n",
    "    ###建立收货地违约率临时表\n",
    "    # tmp_recieve_target = pd.merge(train_recieve,train_target,on=['id'])\n",
    "    # ccc = pd.crosstab(tmp_recieve_target.first_name, tmp_recieve_target.target)\n",
    "    # ccc['recieve违约率'] = ccc[1] / (ccc[0]+0.1)\n",
    "    # ccc.reset_index(inplace=True)\n",
    "    # tmp_recieve_target = pd.merge(tmp_recieve_target, ccc,on = ['first_name'],how='left')\n",
    "    # recieve_score_mean = tmp_recieve_target.groupby(by= ['id'], as_index = False)['recieve违约率'].agg({'recieve违约率_mean':np.mean})\n",
    "    # train_data = pd.merge(train_data,recieve_score_mean,on=['id'],how='left')\n",
    "\n",
    "\n",
    "    train_data = pd.merge(train_data,before_appl_sbm_tm_howmany,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,after_appl_sbm_tm_howmany,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,before_appl_sbm_tm_money_mean,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,after_appl_sbm_tm_money_mean,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,before_appl_sbm_tm_money_max,on=['id'],how='left')\n",
    "    train_data = pd.merge(train_data,after_appl_sbm_tm_money_min,on=['id'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "    # train_data = pd.merge(train_data,before_appl_sbm_tm_howmany_unitprice,on=['id'],how='left')\n",
    "    # train_data = pd.merge(train_data,after_appl_sbm_tm_howmany_unitprice,on=['id'],how='left')\n",
    "    # train_data = pd.merge(train_data,before_appl_sbm_tm_money_mean_unitprice,on=['id'],how='left')\n",
    "    # train_data = pd.merge(train_data,after_appl_sbm_tm_money_mean_unitprice,on=['id'],how='left')\n",
    "    # train_data = pd.merge(train_data,before_appl_sbm_tm_money_max_unitprice,on=['id'],how='left')\n",
    "    # train_data = pd.merge(train_data,after_appl_sbm_tm_money_min_unitprice,on=['id'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_data = train_data.fillna(0)\n",
    "    if IS_OFFLine == False:\n",
    "        train_data = train_data[train_data.appl_sbm_tm >= datetime.datetime(2017,1,1)]\n",
    "        train_data = train_data.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "        print(train_data.shape)\n",
    "    \n",
    "    if IS_OFFLine == True:\n",
    "        dummy_fea = ['sex', 'qq_bound', 'wechat_bound','account_grade']\n",
    "        dummy_df = pd.get_dummies(train_data.loc[:,dummy_fea])\n",
    "        train_data_copy = pd.concat([train_data,dummy_df],axis=1)\n",
    "        train_data_copy = train_data_copy.fillna(0)\n",
    "        vaild_train_data = train_data_copy.drop(dummy_fea,axis=1)\n",
    "        valid_train_train = vaild_train_data[vaild_train_data.appl_sbm_tm < datetime.datetime(2017,4,1)]\n",
    "        valid_train_test = vaild_train_data[vaild_train_data.appl_sbm_tm >= datetime.datetime(2017,4,1)]\n",
    "        valid_train_train = valid_train_train.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "        valid_train_test = valid_train_test.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "        vaild_train_x = valid_train_train.drop(['target'],axis=1)\n",
    "        vaild_test_x = valid_train_test.drop(['target'],axis=1)\n",
    "        redict_result = xgb_feature(vaild_train_x,valid_train_train['target'].values,vaild_test_x,None)\n",
    "        print('valid auc',roc_auc_score(valid_train_test['target'].values,redict_result))\n",
    "        sys.exit(23)\n",
    "\n",
    " \n",
    "    #%%认证表特征\n",
    "    test_auth = pd.read_csv('../AI_risk_test_V3.0/test_auth_info.csv',parse_dates = ['auth_time'])\n",
    "    #注册时是否有时间\n",
    "    test_auth['is_auth_time_authtable'] = test_auth['auth_time'].map(lambda x:0 if ((str(x)=='nan')|(str(x)=='0000-00-00'))  else 1)\n",
    "    #注册时是否有idcard\n",
    "    test_auth['is_idcard_authtable'] = test_auth['id_card'].map(lambda x:0 if str(x)=='nan' else 1)\n",
    "\n",
    "    #注册时是否有phone\n",
    "    test_auth['is_phone_authtable'] = test_auth['phone'].map(lambda x:0 if str(x)=='nan' else 1)\n",
    "    test_auth['auth_time'].replace('0000-00-00','nan',inplace=True)\n",
    "    test_auth['auth_time'] = pd.to_datetime(test_auth['auth_time'])\n",
    "\n",
    "\n",
    "    #%%银行卡特征\n",
    "    test_bankcard = pd.read_csv('../AI_risk_test_V3.0/test_bankcard_info.csv')\n",
    "    bank_name_setlen = test_bankcard.groupby(by= ['id'], as_index= False)['bank_name'].agg({'bank_name_len':lambda x:len(set(x))})\n",
    "    bank_num_len = test_bankcard.groupby(by= ['id'], as_index = False)['tail_num'].agg({'tail_num_len':lambda x:len(x)})\n",
    "    bank_phone_num_setlen = test_bankcard.groupby(by= ['id'], as_index = False)['phone'].agg({'bank_phone_num':lambda x:x.nunique()})\n",
    "    test_bankcard['card_type_score'] = test_bankcard['card_type'].map(lambda x:0.0154925 if x=='信用卡' else 0.02607069)\n",
    "    bank_card_type_score = test_bankcard.groupby(by= ['id'], as_index = False)['card_type_score'].agg({'card_type_score_mean':np.mean})\n",
    "\n",
    "    #%%信誉表特征\n",
    "    test_credit = pd.read_csv('../AI_risk_test_V3.0/test_credit_info.csv')\n",
    "    #额度-使用值\n",
    "    test_credit['can_use_credittable'] = test_credit['quota'] - test_credit['overdraft']\n",
    "\n",
    "    #%%订单表特征\n",
    "    test_order = pd.read_csv('../AI_risk_test_V3.0/test_order_info.csv',parse_dates=['time_order'])\n",
    "    test_order['amt_order_ordertable'] = test_order['amt_order'].map(lambda x:np.nan if ((x == 'NA')| (x == 'null')) else float(x))\n",
    "    test_order['unit_price_ordertable'] = test_order['unit_price'].map(lambda x:np.nan if ((x == 'NA')| (x == 'null')) else float(x))\n",
    "    test_order['time_order_ordertable'] = test_order['time_order'].map(lambda x : pd.lib.NaT if (str(x) == '0' or x == 'NA' or x == 'nan')\n",
    "                                else (datetime.datetime.strptime(str(x),'%Y-%m-%d %H:%M:%S') if ':' in str(x)\n",
    "                                else (datetime.datetime.utcfromtimestamp(int(x[0:10])) + datetime.timedelta(hours = 8))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%%收货地址特征\n",
    "    test_recieve = pd.read_csv('../AI_risk_test_V3.0/test_recieve_addr_info.csv')\n",
    "\n",
    "\n",
    "    #%%target表特征\n",
    "    test_target = pd.read_csv('../AI_risk_test_V3.0/test_list.csv',parse_dates = ['appl_sbm_tm'])\n",
    "\n",
    "\n",
    "    #%%用户表特征\n",
    "    test_user = pd.read_csv('../AI_risk_test_V3.0/test_user_info.csv')\n",
    "    test_user = test_user.drop(['merriage','income','id_card','degree','industry'],axis=1)\n",
    "    # test_user = test_user.drop(['id_card'],axis=1)\n",
    "    test_user['is_hobby_usertable'] = test_user['hobby'].map(lambda x:0 if str(x)=='nan' else 1)\n",
    "    test_user['is_birthday_usertable'] = test_user['birthday'].map(lambda x:0 if str(x)=='nan' else 1)\n",
    "    test_user['birthday'] = test_user['birthday'].map(lambda x:datetime.datetime.strptime(str(x),'%Y-%m-%d') if(re.match('19\\d{2}-\\d{1,2}-\\d{1,2}',str(x)) and '-0' not in str(x)) else pd.lib.NaT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_data = pd.merge(test_target,test_auth,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,test_user,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,test_credit,on=['id'],how='left')\n",
    "    test_data['loan_hour'] = test_data['appl_sbm_tm'].map(lambda x:x.hour)\n",
    "    test_data['loan_day'] = test_data['appl_sbm_tm'].map(lambda x:x.day)\n",
    "    test_data['loan_month'] = test_data['appl_sbm_tm'].map(lambda x:x.month)\n",
    "    test_data['loan_year'] = test_data['appl_sbm_tm'].map(lambda x:x.year)\n",
    "    test_data['nan_num'] = test_data.isnull().sum(axis=1)\n",
    "    test_data['diff_day'] = test_data.apply(lambda row: (row['appl_sbm_tm'] - row['auth_time']).days,axis=1)\n",
    "    test_data['how_old'] = test_data.apply(lambda row: (row['appl_sbm_tm'] - row['birthday']).days,axis=1)\n",
    "    test_data['是否认证时间在借贷时间前'] = test_data.apply(lambda x:0 if (x['is_auth_time_authtable'] == 0) else ( 1 if x['auth_time'] < x['appl_sbm_tm'] else 0),axis=1)\n",
    "    test_data['是否认证时间在借贷时间后'] = test_data.apply(lambda x:0 if (x['is_auth_time_authtable'] == 0) else ( 1 if x['auth_time'] > x['appl_sbm_tm'] else 0),axis=1)\n",
    "    test_data['认证时间在借贷时间前多少天'] = test_data.apply(lambda x:0 if (x['是否认证时间在借贷时间前'] == 0) else (x['appl_sbm_tm'] - x['auth_time']).days,axis=1)\n",
    "    test_data['认证时间在借贷时间后多少天'] = test_data.apply(lambda x:0 if (x['是否认证时间在借贷时间后'] == 0) else (x['auth_time'] - x['appl_sbm_tm']).days,axis=1)\n",
    "    test_data = pd.merge(test_data,bank_name_setlen,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,bank_num_len,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,bank_phone_num_setlen,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,bank_card_type_score,on=['id'],how='left')\n",
    " \n",
    "    #%%为订单表建立临时表\n",
    "    tmp_test_order = pd.merge(test_order, test_target, on = ['id'])\n",
    "    tmp_test_order_before_appl_sbm_tm = tmp_test_order[tmp_test_order.time_order_ordertable < tmp_test_order.appl_sbm_tm]\n",
    "    tmp_test_order_after_appl_sbm_tm = tmp_test_order[tmp_test_order.time_order_ordertable > tmp_test_order.appl_sbm_tm]\n",
    "    before_appl_sbm_tm_howmany = tmp_test_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间前有多少次购买':len})\n",
    "    after_appl_sbm_tm_howmany = tmp_test_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间后有多少次购买':len})\n",
    "    before_appl_sbm_tm_money_mean = tmp_test_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间前有多少次购买':np.mean})\n",
    "    after_appl_sbm_tm_money_mean = tmp_test_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间后有多少次购买':np.mean})\n",
    "    before_appl_sbm_tm_money_max = tmp_test_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间前有多少次购买最大值':np.max})\n",
    "    after_appl_sbm_tm_money_min = tmp_test_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['amt_order_ordertable'].agg({'借贷时间后有多少次购买最小值':np.min})\n",
    "\n",
    "    # before_appl_sbm_tm_howmany_unitprice = tmp_test_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间前有多少次购买unit_price':len})\n",
    "    # after_appl_sbm_tm_howmany_unitprice = tmp_test_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间后有多少次购买unit_price':len})\n",
    "    # before_appl_sbm_tm_money_mean_unitprice = tmp_test_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间前有多少次购买unit_price':np.mean})\n",
    "    # after_appl_sbm_tm_money_mean_unitprice = tmp_test_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间后有多少次购买unit_price':np.mean})\n",
    "    # before_appl_sbm_tm_money_max_unitprice = tmp_test_order_before_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间前有多少次购买最大值unit_price':np.max})\n",
    "    # after_appl_sbm_tm_money_min_unitprice = tmp_test_order_after_appl_sbm_tm.groupby(by=['id'],as_index=False)['unit_price_ordertable'].agg({'借贷时间后有多少次购买最小值unit_price':np.min})\n",
    "\n",
    "\n",
    "    ###建立银行违约率临时表\n",
    "\n",
    "    test_data = pd.merge(test_data,bank_name_score_mean,on=['id'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_data = pd.merge(test_data,before_appl_sbm_tm_howmany,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,after_appl_sbm_tm_howmany,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,before_appl_sbm_tm_money_mean,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,after_appl_sbm_tm_money_mean,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,before_appl_sbm_tm_money_max,on=['id'],how='left')\n",
    "    test_data = pd.merge(test_data,after_appl_sbm_tm_money_min,on=['id'],how='left')\n",
    "\n",
    "    # test_data = pd.merge(test_data,before_appl_sbm_tm_howmany_unitprice,on=['id'],how='left')\n",
    "    # test_data = pd.merge(test_data,after_appl_sbm_tm_howmany_unitprice,on=['id'],how='left')\n",
    "    # test_data = pd.merge(test_data,before_appl_sbm_tm_money_mean_unitprice,on=['id'],how='left')\n",
    "    # test_data = pd.merge(test_data,after_appl_sbm_tm_money_mean_unitprice,on=['id'],how='left')\n",
    "    # test_data = pd.merge(test_data,before_appl_sbm_tm_money_max_unitprice,on=['id'],how='left')\n",
    "    # test_data = pd.merge(test_data,after_appl_sbm_tm_money_min_unitprice,on=['id'],how='left')\n",
    "\n",
    "    test_data = test_data.drop(['appl_sbm_tm','id','auth_time','phone','birthday','hobby','id_card'],axis=1)\n",
    "    test_data['target'] = -1\n",
    "\n",
    "\n",
    "    dummy_fea = ['sex', 'qq_bound', 'wechat_bound','account_grade']\n",
    "    train_test_data = pd.concat([train_data,test_data],axis=0,ignore_index = True)\n",
    "    train_test_data = train_test_data.fillna(0)\n",
    "    dummy_df = pd.get_dummies(train_test_data.loc[:,dummy_fea])\n",
    "\n",
    "    train_test_data = pd.concat([train_test_data,dummy_df],axis=1)\n",
    "    train_test_data = train_test_data.drop(dummy_fea,axis=1)\n",
    "    \n",
    "    train_train = train_test_data.iloc[:train_data.shape[0],:]\n",
    "    test_test = train_test_data.iloc[train_data.shape[0]:,:]\n",
    "    \n",
    "    \n",
    "    train_train_x = train_train.drop(['target'],axis=1)\n",
    "    test_test_x = test_test.drop(['target'],axis=1)\n",
    "\n",
    "\n",
    "    predict_result = xgb_feature(train_train_x,train_train['target'].values,test_test_x,None)\n",
    "\n",
    "    ans = pd.read_csv('../AI_risk_test_V3.0/test_list.csv',parse_dates = ['appl_sbm_tm'])\n",
    "    ans['PROB'] = predict_result\n",
    "    ans = ans.drop(['appl_sbm_tm'],axis=1)\n",
    "    minmin, maxmax = min(ans['PROB']),max(ans['PROB'])\n",
    "    ans['PROB'] = ans['PROB'].map(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    ans['PROB'] = ans['PROB'].map(lambda x:'%.4f' % x)\n",
    "    ans.to_csv('../result/rebuild.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
