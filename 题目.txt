解答过程请在jupyter中进行，每一步骤执行后，都请保留结果；请使用“qwen/Qwen-7B-Chat”模型，指定 v1.1.9 版本，如：

tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", revision='v1.1.9', trust_remote_code=True)



题目描述:

你将使用主办方提供的大语言模型（Qwen），通过高效参数微调技术对大模型进行微调。本次古籍文献的命名实体识别评测，通过发布全新的基于“二十四史”的训练和测试数据集，提供统一的评测提交平台，以此推动技术的突破和发展，助力古籍资源的智能开发与利用。



任务描述:

命名实体识别（Name Entity Recognition）任务旨在自动识别出文本中人名、地名、机构名等事件基本构成要素的重要实体。古籍文献的命名实体识别是正确分析处理古汉语文本的基础步骤，也是深度挖掘、组织人文知识的重要前提。近年来，学界已有多项研究关注史籍、方志、诗词、中医等等类目的古籍命名实体识别，构建了一些针对垂直领域的小型标注数据集，实体标注的体系和规范有所差异，识别范围往往由三种基本实体类别扩充至人文计算研究所需的多种特殊类别，如书名、药物名、疾病名、动植物名等。这些研究所构建针对特殊领域的小型标注数据集，实体类型有差异。总体而言，古籍命名实体识别任务仍旧缺乏可用于模型训练以及评测的公开数据资源，阻碍了技术的长足发展。另一方面，古文字词含义的多样性、行文结构的连续性以及多用繁体字、无句读等特点，也增加了古籍文献命名实体识别任务的复杂和困难程度。因此，我们基于“二十四史”，设计了涵盖人名、书名、官职名等多项的实体知识体系，建构了覆盖多个朝代的历时、跨领域的数据资源，完善古籍命名实体识别任务的建立。本次古籍文献的命名实体识别评测，通过发布全新的基于“二十四史”的训练和测试数据集，提供统一的评测提交平台，以此推动技术的突破和发展，助力古籍资源的智能开发与利用。



评测数据:

本次评测提供官方评测数据集“古籍命名实体识别 2023”(GuNER 2023)，由北京大学数字人文研究中心组织标注，语料来源是网络上公开的部分中国古代正史纪传文本。数据包括供参赛队伍进行模型训练与调优的训练集，以及评测参赛队伍的模型性能的封闭测试数据集。同时，各参赛队伍可以自行使用其他公开的人工标注数据集和伪造数据集。训练集以“二十四史”为基础语料，包含 13 部书中的 22 卷语料，随机截断为长度约 100 字的片段，标注了人名（PER）、书名（BOOK）、官职名（OFI）三种实体，总计 15.4 万字（计标点）。各实体的标注要求详见标注规范。评测数据集格式为文本文件，参赛队伍可根据模型需要进行转化处理。其中训练集数据样例如下所示，每行为二十四史原文中的一个段落，段中每一个实体以“{ }”标识，“|”后为实体类别。测试集数据禁止包含原文内容，参赛队伍需要提交在测试集文本上的实体识别结果文件，格式与训练集一致

 

"""

{輔元|PER}兄{希元|PER}，{高宗|PER}時洛州{司法參軍|OFI}，{章懷太子|PER}召令與{洗馬|OFI}{劉訥言|PER}等注解{范曄|PER}{後漢書|BOOK}，行於代。先{輔元|PER}卒。

{友倫|PER}幼亦明敏，通{論語|BOOK}、{小學|BOOK}，曉音律。{存|PER}已死，{太祖|PER}以{友倫|PER}為{元從馬軍指揮使|OFI}，表{右威武將軍|OFI}。

"""



本次评测的测试数据集采用封闭方式给出，即仅给定原古文文本，需要参赛队伍训练模型对文本中的命名实体进行自动识别和标注，并将结果文件上传至在线评测平台（jupyter点击跳转后的目录下，最终结果形式参考out_put.ipynb），获取评测指标得分，得分以平均值计算。



备注

虚拟机内已安装 python 管理工具 conda，已创建python虚拟环境 py3.11，内已安装好必要的包；jupyter 运行在虚拟机下

```

(base) [root@localhost ~]# conda env list

# conda environments:

#

base * /root/miniconda3

py3.11 /root/miniconda3/envs/py3.11

```



data ---数据文件夹

->/add_data/unlabel_24_history.txt --- 预训练数据

->/test_data/GuNER2023_test_public.txt --- 测试集数据

模型操作jupyter 提供的Qwen中的Readme可查看参考

预测值和期望值的比值为 f1，针对书名，人名，地名等分别求f1，并求出以上3个f1的平均值作为最终评判结果

 

分细则方案

根据赛题要求，评分分为三个主要步骤，每个步骤包含多个具体任务。评分细则总分为100分，各步骤具体评分如下：

 

步骤一：数据预处理（总分：10分）

  1）使用Python代码读取数据和查看预训练数据（unlabel_24_history.txt）（5分）

  2）把预训练数据（unlabel_24_history.txt）对测试集数据（GuNER2023_test_public.txt ）进行去重操作（5分）

步骤二：训练预处理与分析训练（总分：20分）

1）预训练数据（unlabel_24_history.txt）转换为BIO形式 （10分）

 BIO样例：

 """

帝 O

曰 O

： O

「 O

玄 B-PER

"""

2）预训练数据（unlabel_24_history.txt）处理为GlobalPointer的输入形式（10分）

步骤三：模型微调与训练（总分：70分）

  1）超参数调整：自行选择合适的微调算法（如LoRA）对模型进行超参数调整（15分）

  2）模型预测

   ·不加交叉验证调优模型参数（10分）

   ·交叉验证调优模型参数（10分）

  4）模型预测与提交

   ·在测试集上进行预测（预测结果文件命名为：best_res.txt）（5分）

   ·提交预测结果,系统给出对应的模型评分（提交训练好的模型到根目录）（30分）

   （预测结果以书名，人名，地名3个因素的f1-score平均值为依据打分，分值位于前5名得30分、第6-10名20分、15-20名10分、余下为5分）

 

F1-Score 计算公式说明

F1-Score 是一个用于评估分类模型性能的指标，特别适用于不平衡数据集。它结合了精确率（Precision）和召回率（Recall），并取它们的调和平均数。

精确率（Precision）

精确率表示在所有被模型预测为正类的样本中，实际为正类的比例。计算公式为：

精确率 = 真正例数 / (真正例数 + 假正例数)

其中：

· 真正例数（True Positives, TP）是被正确预测为正类的样本数量。

· 假正例数（False Positives, FP）是被错误预测为正类的样本数量。

召回率（Recall）

召回率表示在所有实际为正类的样本中，被模型正确预测为正类的比例。计算公式为：

召回率 = 真正例数 / (真正例数 + 假负例数)

其中：

· 假负例数（False Negatives, FN）是实际为正类但被错误预测为负类的样本数量。

F1-Score

F1-Score 是精确率和召回率的调和平均数，计算公式为：

F1-Score = 2 (精确率 召回率) / (精确率 + 召回率)